---
layout: default
---

# Call for Papers

We invite submissions on any aspect of backdoor attacks and defenses in machine learning, which includes but not limited to:

- Novel backdoor attacks against ML systems, including CV, NLP, ML models in cyber-physical systems, etc.
- Detecting backdoored models under different threat models, such as having limited clean data or no data, no access to model weights, using attack samples, etc.
- Eliminating backdoors in attacked models under different settings, such as limited access or no access to the original training/test data
- Certification/verification methods against backdoor attacks with guarantees
- Real-world or physical backdoor attacks in deployed systems, such as autonomous driving systems, facial recognition systems, etc.
- Hardware-based backdoor attacks in ML
- Backdoors in distributed learning, federated learning, reinforcement learning, etc.
- Theoretical understanding of backdoor attacks in machine learning
- Explainable and interpretable AI in backdoor scenario
- Futuristic concerns on trustworthiness and societal impact of ML systems regarding backdoor threats
- Exploration of the relation among backdoors, adversarial robustness, fairness
- New applications of backdoors in other scenarios, such as watermarking ML property, boosting privacy attacks, etc.

The workshop will employ a double-anonymous review process. Each submission will be evaluated based on the following criteria:

- Soundness of the methodology
- Novelty
- Relevance to the workshop
- Societal impacts

We only consider submissions that havenâ€™t been published in any peer-reviewed venue, including ICLR 2023 conference. We allow dual submission with other workshops or conferences. The workshop is non-archival and will not have any official proceedings. All accepted papers will be allocated either a virtual poster presentation, or a virtual talk slot.

